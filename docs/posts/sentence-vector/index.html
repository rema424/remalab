<!doctype html><html lang=ja-jp data-theme><head><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N95L2VD');</script><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>【2020年11月版】PyTorchとBERTで文章の分散表現（ベクトル）を取得する - RemaLab</title><meta name=description content="PyTorchでBERTを利用して文章の分散表現（ベクトル）を取得する方法を確認します"><link rel=icon type=image/x-icon href=https://rema424.github.io/remalab/favicon.ico><link rel=apple-touch-icon-precomposed href=https://rema424.github.io/remalab/favicon.png><link rel=stylesheet href=https://rema424.github.io/remalab/css/style.min.9aff271207ab9adf80eba2147d0c7fecd4fee20373798aff107adfef419fc680.css integrity="sha256-mv8nEgermt+A66IUfQx/7NT+4gNzeYr/EHrf70GfxoA="><link rel=stylesheet href="https://rema424.github.io/remalab/css/main.css?rnd=1605326966"><meta property="og:title" content="【2020年11月版】PyTorchとBERTで文章の分散表現（ベクトル）を取得する"><meta property="og:description" content="PyTorchでBERTを利用して文章の分散表現（ベクトル）を取得する方法を確認します"><meta property="og:type" content="article"><meta property="og:url" content="https://rema424.github.io/remalab/posts/sentence-vector/"><meta property="article:published_time" content="2020-11-13T21:13:05+09:00"><meta property="article:modified_time" content="2020-11-13T21:13:05+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【2020年11月版】PyTorchとBERTで文章の分散表現（ベクトル）を取得する"><meta name=twitter:description content="PyTorchでBERTを利用して文章の分散表現（ベクトル）を取得する方法を確認します"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N95L2VD" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><h1 class=site-title><a href=/remalab>RemaLab</a></h1><nav></nav></header><main id=main tabindex=-1><article class=post><header class=post-header><h1 class=post-title>【2020年11月版】PyTorchとBERTで文章の分散表現（ベクトル）を取得する</h1></header><div class=content><p>日本語における自然言語処理のライブラリ整備がここ1年で大きく進んだみたいです。
これに伴って情報の古い記事も多くなったように感じられたので記しておきます。</p><p>以前はBERTで日本語を扱う場合は形態素解析やBERTトークン化を自身で行う必要があったようですが、今はライブラリに一任できます。環境構築も非常に簡単でした。</p><h2 id=setup>setup</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e># python3.8のインストール（2020年11月現在、3.9だとエラーになるライブラリが多いため3.8を利用する）</span>
brew install python@3.8

<span style=color:#75715e># 仮想環境の作成</span>
python3 -m venv venv

<span style=color:#75715e># 仮想環境をアクティベート</span>
. venv/bin/activate

<span style=color:#75715e># pipをアップグレード</span>
python -m pip install --upgrade pip

<span style=color:#75715e># pytorch（機械学習ライブラリ）のインストール</span>
pip install torch

<span style=color:#75715e># transformers（自然言語処理における学習済みモデル提供ライブラリ）のインストール</span>
pip install transformers

<span style=color:#75715e># numpy（行列計算ライブラリ）のインストール</span>
pip install numpy

<span style=color:#75715e># fugashi（形態素解析器）のインストール</span>
pip install fugashi

<span style=color:#75715e># ipadic（形態素解析用辞書）のインストール</span>
pip install ipadic

<span style=color:#75715e># mojimoji（日本語文字列の半角・全角変換ライブラリ）のインストール</span>
pip install mojimoji
</code></pre></div><h2 id=サンプルコード>サンプルコード</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>import</span> torch
<span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertModel
<span style=color:#f92672>from</span> transformers.tokenization_bert_japanese <span style=color:#f92672>import</span> BertJapaneseTokenizer
<span style=color:#f92672>import</span> re
<span style=color:#f92672>import</span> mojimoji
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>from</span> pprint <span style=color:#f92672>import</span> pprint <span style=color:#66d9ef>as</span> p


<span style=color:#75715e># GPUが利用できるか確認</span>
p(torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available())

<span style=color:#75715e># GPUとCPUのどちらを利用するか判定</span>
device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda:0&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>)
p(device)

<span style=color:#75715e># トークナイザーの読み込み</span>
tokenizer <span style=color:#f92672>=</span> BertJapaneseTokenizer<span style=color:#f92672>.</span>from_pretrained(
    <span style=color:#e6db74>&#34;cl-tohoku/bert-base-japanese-whole-word-masking&#34;</span>
)

<span style=color:#75715e># 学習済みモデルの読み込み</span>
model <span style=color:#f92672>=</span> BertModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;cl-tohoku/bert-base-japanese-whole-word-masking&#34;</span>)


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preprocess</span>(text):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    日本語文字列の前処理を行う
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    <span style=color:#75715e># 全角文字を半角に変換</span>
    text <span style=color:#f92672>=</span> mojimoji<span style=color:#f92672>.</span>zen_to_han(text)

    <span style=color:#75715e># 空白と改行を削除し、コンマとピリオドを句点と読点に変換</span>
    text <span style=color:#f92672>=</span> (
        text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
        <span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
        <span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;、&#34;</span>)
        <span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;.&#34;</span>, <span style=color:#e6db74>&#34;。&#34;</span>)
        <span style=color:#f92672>.</span>strip()
    )

    <span style=color:#75715e># 数字を#に変換</span>
    <span style=color:#66d9ef>while</span> bool(re<span style=color:#f92672>.</span>search(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#34;[0-9]&#34;</span>, text)):
        text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;[0-9]{5,}&#34;</span>, <span style=color:#e6db74>&#34;#####&#34;</span>, text)
        text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;[0-9]{4}&#34;</span>, <span style=color:#e6db74>&#34;####&#34;</span>, text)
        text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;[0-9]{3}&#34;</span>, <span style=color:#e6db74>&#34;###&#34;</span>, text)
        text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;[0-9]{2}&#34;</span>, <span style=color:#e6db74>&#34;##&#34;</span>, text)
        text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;[0-9]{1}&#34;</span>, <span style=color:#e6db74>&#34;#&#34;</span>, text)

    <span style=color:#75715e># 半角文字を全角に変換</span>
    <span style=color:#66d9ef>return</span> mojimoji<span style=color:#f92672>.</span>han_to_zen(text)


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>split_to_sentences</span>(text):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    文章を文単位で分割しリストで返却する
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>&#34;。&#34;</span>, <span style=color:#e6db74>&#34;。</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, text)
    <span style=color:#66d9ef>return</span> [s <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>) <span style=color:#66d9ef>if</span> s <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#34;&#34;</span>]


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>aggregate_tensor</span>(embeddings, pooling_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;REDUCE_MEAN_MAX&#34;</span>):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    文ベクトルを集計して文章ベクトルを返却する
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    <span style=color:#66d9ef>if</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MEAN&#34;</span>:
        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>mean(embeddings, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>elif</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MAX&#34;</span>:
        max, _ <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(embeddings, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
        <span style=color:#66d9ef>return</span> max
    <span style=color:#66d9ef>elif</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MEAN_MAX&#34;</span>:
        max, _ <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(embeddings, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
        mean <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(embeddings, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>hstack((max, mean))


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>aggregate_numpy</span>(embeddings, pooling_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;REDUCE_MEAN_MAX&#34;</span>):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    文ベクトルを集計して文章ベクトルを返却する
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    <span style=color:#66d9ef>if</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MEAN&#34;</span>:
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(embeddings, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>elif</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MAX&#34;</span>:
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>max(embeddings, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>elif</span> pooling_strategy <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;REDUCE_MEAN_MAX&#34;</span>:
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>r_[np<span style=color:#f92672>.</span>max(embeddings, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>), np<span style=color:#f92672>.</span>mean(embeddings, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)]


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>embedding</span>(text):
    <span style=color:#75715e># 前処理</span>
    text <span style=color:#f92672>=</span> preprocess(text)

    <span style=color:#75715e># 文単位に分割</span>
    sentences <span style=color:#f92672>=</span> split_to_sentences(text)

    <span style=color:#75715e># BERTトークン化</span>
    encoded <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>batch_encode_plus(
        sentences, padding<span style=color:#f92672>=</span>True, add_special_tokens<span style=color:#f92672>=</span>True
    )

    <span style=color:#75715e># BERTトークンID列を抽出</span>
    input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(encoded[<span style=color:#e6db74>&#34;input_ids&#34;</span>], device<span style=color:#f92672>=</span>device)

    <span style=color:#75715e># BERTの最大許容トークン数が512なので超える場合は切り詰める</span>
    input_ids <span style=color:#f92672>=</span> input_ids[:, :<span style=color:#ae81ff>512</span>]

    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():  <span style=color:#75715e># 勾配計算なし</span>
        <span style=color:#75715e># 単語ベクトルを計算</span>
        outputs <span style=color:#f92672>=</span> model(input_ids)

    <span style=color:#75715e># 最終層の隠れ状態ベクトルを取得</span>
    last_hidden_states <span style=color:#f92672>=</span> outputs[<span style=color:#ae81ff>0</span>]

    <span style=color:#75715e># 各文における[CLS]トークンの単語ベクトルを抽出（これを文ベクトルとみなす）</span>
    vecs <span style=color:#f92672>=</span> last_hidden_states[:, <span style=color:#ae81ff>0</span>, :]

    <span style=color:#75715e># 文ベクトルから文章ベクトルを計算</span>
    vec <span style=color:#f92672>=</span> aggregate_tensor(vecs)

    <span style=color:#75715e># 保存容量削減のため型変換</span>
    <span style=color:#75715e># vec = vec.type(torch.float16)</span>

    <span style=color:#75715e># データをgpuからcpuに載せ替え</span>
    <span style=color:#75715e># vec = vec.cpu()</span>

    <span style=color:#75715e># numpy配列に変換</span>
    <span style=color:#75715e># vec = vec.numpy()</span>

    <span style=color:#66d9ef>return</span> vec


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cos_similarity</span>(x, y, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-8</span>):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    コサイン類似度を計算する
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    nx <span style=color:#f92672>=</span> x <span style=color:#f92672>/</span> (torch<span style=color:#f92672>.</span>sqrt(torch<span style=color:#f92672>.</span>sum(x <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)) <span style=color:#f92672>+</span> eps)
    ny <span style=color:#f92672>=</span> y <span style=color:#f92672>/</span> (torch<span style=color:#f92672>.</span>sqrt(torch<span style=color:#f92672>.</span>sum(y <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)) <span style=color:#f92672>+</span> eps)
    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>dot(nx, ny)


<span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
    text_a <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>夏目 漱石（なつめ そうせき、1867年2月9日〈慶応3年1月5日〉 - 1916年〈大正5年〉12月9日）は、日本の小説家、評論家、英文学者、俳人。本名は夏目 金之助（なつめ きんのすけ）。俳号は愚陀仏。明治末期から大正初期にかけて活躍した近代日本文学の頂点に立つ作家の一人である。代表作は『吾輩は猫である』『坊っちゃん』『三四郎』『それから』『こゝろ』『明暗』など。明治の文豪として日本の千円紙幣の肖像にもなり、講演録「私の個人主義」も知られている。漱石の私邸に門下生が集った会は木曜会と呼ばれた。
</span><span style=color:#e6db74>江戸の牛込馬場下横町（現在の東京都新宿区喜久井町）出身。大学時代に正岡子規と出会い、俳句を学ぶ。帝国大学（のちの東京帝国大学、現在の東京大学）英文科卒業後、松山で愛媛県尋常中学校教師、熊本で第五高等学校教授などを務めたあと、イギリスへ留学。帰国後は東京帝国大学講師として英文学を講じ、講義録には『文学論』がある。
</span><span style=color:#e6db74>講師の傍ら『吾輩は猫である』を雑誌『ホトトギス』に発表。これが評判になり『坊っちゃん』『倫敦塔』などを書く。その後朝日新聞社に入社し、『虞美人草』『三四郎』『それから』などを掲載。当初は余裕派と呼ばれた。「修善寺の大患」後は、『行人』『こゝろ』『道草』などを執筆。「則天去私（そくてんきょし）」の境地に達したといわれる。晩年は胃潰瘍に悩まされ、『明暗』が絶筆となった。
</span><span style=color:#e6db74>&#34;&#34;&#34;</span>

    text_b <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>森 鷗外（もり おうがい、文久2年1月19日[1]〈1862年2月17日[2][注釈 1]〉 - 1922年〈大正11年〉7月9日）は、日本の明治・大正期の小説家、評論家、翻訳家、陸軍軍医（軍医総監＝中将相当）、官僚（高等官一等）。位階勲等は従二位・勲一等・功三級、医学博士、文学博士。本名は森 林太郎（もり りんたろう）。
</span><span style=color:#e6db74>石見国津和野（現：島根県津和野町）出身。東京大学医学部[注釈 2]卒業。
</span><span style=color:#e6db74>大学卒業後、陸軍軍医になり、陸軍省派遣留学生としてドイツでも軍医として4年過ごした。帰国後、訳詩編「於母影」、小説「舞姫」、翻訳「即興詩人」を発表する一方、同人たちと文芸雑誌『しがらみ草紙』を創刊して文筆活動に入った。その後、日清戦争出征や小倉転勤などにより一時期創作活動から遠ざかったものの、『スバル』創刊後に「ヰタ・セクスアリス」「雁」などを発表。乃木希典の殉死に影響されて「興津弥五右衛門の遺書」を発表後、「阿部一族」「高瀬舟」など歴史小説や史伝「澁江抽斎」なども執筆した。
</span><span style=color:#e6db74>晩年、帝室博物館（現在の東京国立博物館・奈良国立博物館・京都国立博物館等）総長や帝国美術院（現：日本芸術院）初代院長なども歴任した。    
</span><span style=color:#e6db74>&#34;&#34;&#34;</span>

    vec_a <span style=color:#f92672>=</span> embedding(text_a)
    vec_b <span style=color:#f92672>=</span> embedding(text_b)
    sim <span style=color:#f92672>=</span> cos_similarity(vec_a, vec_b)<span style=color:#f92672>.</span>item()

    p(vec_a<span style=color:#f92672>.</span>shape)
    p(vec_a<span style=color:#f92672>.</span>dtype)
    p(vec_a[:<span style=color:#ae81ff>5</span>])

    p(vec_b<span style=color:#f92672>.</span>shape)
    p(vec_b<span style=color:#f92672>.</span>dtype)
    p(vec_b[:<span style=color:#ae81ff>5</span>])

    p(sim)
</code></pre></div><h2 id=サンプルコードの実行>サンプルコードの実行</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>$ python main.py

False
device<span style=color:#f92672>(</span>type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cpu&#39;</span><span style=color:#f92672>)</span>
torch.Size<span style=color:#f92672>([</span>1536<span style=color:#f92672>])</span>
torch.float32
tensor<span style=color:#f92672>([</span>-0.1967,  0.9231,  0.3203,  0.4452,  0.1927<span style=color:#f92672>])</span>
torch.Size<span style=color:#f92672>([</span>1536<span style=color:#f92672>])</span>
torch.float32
tensor<span style=color:#f92672>([</span>-0.1913,  0.5956,  0.1263,  0.3581, -0.2445<span style=color:#f92672>])</span>
0.9690771102905273
</code></pre></div></div><div class=post-info><div class=post-date>2020-11-13</div><div class=post-taxonomies></div></div></article></main><footer class=common-footer><div class=common-footer-bottom><ul class=footer-menu><li><a href=https://rema424.github.io/remalab/info/privacy/ title=プライバシーポリシー>プライバシーポリシー</a></li></ul><div class=copyright><p>© rema, 2020<br>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.</p></div></div></footer></div></body></html>